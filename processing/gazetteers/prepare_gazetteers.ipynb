{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from pandarallel import pandarallel\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_candranker(gazname, unique_placenames_array):\n",
    "    \"\"\"\n",
    "    This function returns the unique alternate names in a given gazetteer\n",
    "    in the format required by DeezyMatch candidate ranker.\"\"\"\n",
    "    with open(\"../../datasets/candidate_mentions_sets/\" + gazname + \".txt\", \"w\") as fw:\n",
    "        for pl in unique_placenames_array:\n",
    "            pl = pl.strip()\n",
    "            if pl:\n",
    "                if not \"wikipedia\" in pl: # Remove altnames that are wikiURLs (from geonames)\n",
    "                    if not any(char.isdigit() for char in pl):\n",
    "                        if not '\"' in pl:\n",
    "                            fw.write(pl.strip() + \"\\t0\\tfalse\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing WikiGazetteer\n",
    "\n",
    "This step assumes the user has already run [this notebook](https://github.com/Living-with-machines/LwM_SIGSPATIAL2020_ToponymMatching/blob/master/processing/gazetteers/generate_wikigazetteers.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_languages = [\"en\", \"es\", \"el\"]\n",
    "\n",
    "for language in gaz_languages:\n",
    "    wiki_lang = pd.read_pickle(\"../resources/wikigaz_\" + language + \"_basic.pkl\")\n",
    "    wiki_lang = wiki_lang.drop(columns=['source'])\n",
    "    wiki_lang.to_pickle(\"../../datasets/gazetteers/wikigaz_\" + language + \".pkl\")\n",
    "    wiki_lang = wiki_lang.drop_duplicates(subset = ['altname', 'lat', 'lon'])\n",
    "    unique_placenames_array = list(set(list(np.array(wiki_lang[\"altname\"]))))\n",
    "    format_for_candranker(\"wikigaz_\" + language, unique_placenames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in gaz_languages:\n",
    "    wgdf = pd.read_pickle(\"../resources/wikigaz_\" + language + \"_basic.pkl\")\n",
    "    wgdf = wgdf[wgdf[\"source\"].isin(['wikimain', 'geonamesmain', 'geonamesascii', 'geonamesalt', 'wikiredirect'])]\n",
    "    wgdf = wgdf[wgdf[\"altname\"].str.len() < 30]\n",
    "    wgdf['lat'] = wgdf['lat'].astype(float)\n",
    "    wgdf['lon'] = wgdf['lon'].astype(float)\n",
    "    wgdf = wgdf[wgdf['lat'].notna()]\n",
    "    wgdf = wgdf[wgdf['lon'].notna()]\n",
    "    wgdf = wgdf.rename(columns={\"altname\": \"name\", \"pid\": \"wikititle\", \"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "    wgdf.to_pickle(\"../resources/wikiGaz_\" + language + \"_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing Pleiades\n",
    "\n",
    "Download Pleiades gazetteer [from here](http://atlantides.org/downloads/pleiades/dumps/pleiades-names-latest.csv.gz), unzip it, and store it in `toponym_matching/processing/resources/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../resources/pleiades-names-latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter only interested in entries written in Greek alphabet, and format them according to the format needed as input for DeezyMatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternatename = []\n",
    "pid = []\n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"nameLanguage\"] == \"grc\" or row[\"nameLanguage\"] == \"el\":\n",
    "        if type(row[\"nameAttested\"]) == str and type(row[\"reprLat\"]) == float and type(row[\"reprLong\"]):\n",
    "            toponym = row[\"nameAttested\"]\n",
    "            alternatename.append(toponym)\n",
    "            pid.append(row[\"pid\"])\n",
    "            lat.append(row[\"reprLat\"])\n",
    "            lon.append(row[\"reprLong\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleiades_gaz = pd.DataFrame()\n",
    "pleiades_gaz['altname'] = alternatename\n",
    "pleiades_gaz['pid'] = pid\n",
    "pleiades_gaz['lat'] = lat\n",
    "pleiades_gaz['lon'] = lon\n",
    "            \n",
    "pleiades_gaz['lat'] = pd.to_numeric(pleiades_gaz['lat'], errors = 'coerce')\n",
    "pleiades_gaz['lon'] = pd.to_numeric(pleiades_gaz['lon'], errors = 'coerce')\n",
    "pleiades_gaz.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleiades_gaz.to_pickle(\"../../datasets/gazetteers/pleiades.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(pleiades_gaz[\"altname\"]))))\n",
    "format_for_candranker(\"pleiades\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Combine Pleiades and WikiGazEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_wgaz_pleiades = pd.concat([pd.read_pickle(\"../../datasets/gazetteers/pleiades.pkl\"), pd.read_pickle(\"../../datasets/gazetteers/wikigaz_el.pkl\")])\n",
    "greek_wgaz_pleiades = greek_wgaz_pleiades.drop_duplicates(subset = ['altname', 'lat', 'lon'])\n",
    "greek_wgaz_pleiades.to_pickle(\"../../datasets/gazetteers/wikigaz_pleiades_el.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(greek_wgaz_pleiades[\"altname\"].unique()))))\n",
    "format_for_candranker(\"wikigaz_pleiades_el\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing HGIS de las Indias\n",
    "\n",
    "Do the following four steps only once:\n",
    "1. Download gazetteer from https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/FUSJD3/DK27GE&version=2.0\n",
    "2. Unzip and file in `toponym_matching/processing/resources/`.\n",
    "3. Convert zip to df (uncomment and run cell below).\n",
    "4. Store dataframe in `toponym_matching/processing/resources/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and run this only once. Change filename accordingly:\n",
    "# import simpledbf\n",
    "# dbf = simpledbf.Dbf5('../../resources/gazetteer-2019-03-28/gazetteer-2019-03-28.dbf')\n",
    "# df = dbf.to_dataframe()\n",
    "# df.to_pickle(\"../../resources/hgis_de_las_indias.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../resources/hgis_de_las_indias.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias = pd.DataFrame()\n",
    "df_indias[\"altname\"] = df[\"label\"]\n",
    "df_indias[\"pid\"] = df[\"gz_id\"]\n",
    "df_indias[\"lat\"] = df[\"lat\"]\n",
    "df_indias[\"lon\"] = df[\"lon\"]\n",
    "            \n",
    "df_indias['lat'] = pd.to_numeric(df_indias['lat'], errors = 'coerce')\n",
    "df_indias['lon'] = pd.to_numeric(df_indias['lon'], errors = 'coerce')\n",
    "df_indias.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias = df_indias.drop_duplicates(subset=[\"altname\", \"pid\", \"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias.to_pickle(\"../../datasets/gazetteers/hgisindias.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(df_indias[\"altname\"]))))\n",
    "format_for_candranker(\"hgisindias\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Combine HGISindias and WikiGazES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wges = pd.read_pickle(\"../../datasets/gazetteers/wikigaz_es.pkl\")\n",
    "wges = wges.rename(columns={\"wikititle\": \"pid\"})\n",
    "\n",
    "es_wgaz_hgisindias = pd.concat([pd.read_pickle(\"../../datasets/gazetteers/hgisindias.pkl\"), wges])\n",
    "es_wgaz_hgisindias = es_wgaz_hgisindias.drop_duplicates(subset = ['altname', 'lat', 'lon'])\n",
    "es_wgaz_hgisindias.to_pickle(\"../../datasets/gazetteers/wikigaz_hgisindias_es.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(es_wgaz_hgisindias[\"altname\"]))))\n",
    "format_for_candranker(\"wikigaz_hgisindias_es\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
