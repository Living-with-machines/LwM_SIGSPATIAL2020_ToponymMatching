{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from pandarallel import pandarallel\n",
    "import sqlite3\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import unicodedata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing WikiGazetteer\n",
    "\n",
    "WikiGazetteer is a gazetteer based on Wikipedia and enriched with Geonames data. To build a WikiGazetteer into a MySQL database for a specific Wikipedia language and version, follow [these instructions](https://github.com/Living-with-machines/lwm_GIR19_resolving_places/tree/master/gazetteer_construction). This notebook assumes the user already has created WikiGazetteers for English, Spanish, and Greek, which are stored in a MySQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create minimal Wikigazetteer\n",
    "\n",
    "The following cells create a minimal WikiGazetteer. It results in a dataframe (which is stored in `toponym_matching/datasets/gazetteers/` as a pickle file), where each row corresponds to an altname-location pair. The dataframe has the following fields:\n",
    "* **altname:** alternate name of a location.\n",
    "* **pid:** persistent identifier of the location (e.g. the wikipedia title in WikiGazetteer).\n",
    "* **lat:** latitude of the location.\n",
    "* **lon:** longitude of the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minimal_gaz(gazetteer):\n",
    "    gazDB = \"\"\n",
    "    cursorGaz = \"\"\n",
    "    try:\n",
    "        gazDB = mysql.connector.connect(\n",
    "                host='localhost',\n",
    "                database=gazetteer,\n",
    "                user='testGazetteer',\n",
    "                password='1234')\n",
    "        if gazDB.is_connected():\n",
    "            cursorGaz = gazDB.cursor(buffered=True)\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to MySQL\", e)\n",
    "        \n",
    "    cursorGaz.execute(\"\"\"SELECT altname, wiki_title, lat, lon FROM altname\n",
    "                         JOIN location ON altname.main_id = location.id\"\"\")\n",
    "    locs = cursorGaz.fetchall()\n",
    "    df = pd.DataFrame(locs, columns =['altname', 'pid', 'lat', 'lon'])\n",
    "    \n",
    "    # Close DB connection:\n",
    "    if (gazDB.is_connected()):\n",
    "        cursorGaz.close()\n",
    "        gazDB.close()\n",
    "    \n",
    "    gaznames = {\"wikiGazES\":\"wikigaz_es\",\n",
    "            \"wikiGazEL\":\"wikigaz_el\",\n",
    "            \"wikiGazetteer\": \"wikigaz_en\"}\n",
    "    df.to_pickle(\"../../datasets/gazetteers/\" + gaznames[gazetteer] + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimal gazetteer: the argument is the name of the MySQL database\n",
    "create_minimal_gaz(\"wikiGazES\")\n",
    "create_minimal_gaz(\"wikiGazEL\")\n",
    "create_minimal_gaz(\"wikiGazetteer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all distinct candidate mentions from gazetteer\n",
    "\n",
    "Find all unique alternate names in the gazetteer (i.e. the candidate mentions), and format them according to the format needed as input for DeezyMatch:\n",
    "```\n",
    "Namps Maisnil\t0\tfalse\n",
    "Municipio de Wedington\t0\tfalse\n",
    "Corcuera\t0\tfalse\n",
    "Sin√©matiali\t0\tfalse\n",
    "Trakay\t0\tfalse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_candranker(gazname, unique_placenames_array):\n",
    "    with open(\"../../datasets/candidate_mentions_sets/\" + gazname + \".txt\", \"w\") as fw:\n",
    "        for pl in unique_placenames_array:\n",
    "            pl = pl.strip()\n",
    "            if pl:\n",
    "                if not \"wikipedia\" in pl: # Remove altnames that are wikiURLs (from geonames)\n",
    "                    if not any(char.isdigit() for char in pl):\n",
    "                        if not '\"' in pl:\n",
    "                            fw.write(pl.strip() + \"\\t0\\tfalse\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placenames(gazetteer):\n",
    "    gazDB = \"\"\n",
    "    cursorGaz = \"\"\n",
    "    try:\n",
    "        gazDB = mysql.connector.connect(\n",
    "                host='localhost',\n",
    "                database=gazetteer,\n",
    "                user='testGazetteer',\n",
    "                password='1234')\n",
    "        if gazDB.is_connected():\n",
    "            cursorGaz = gazDB.cursor(buffered=True)\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to MySQL\", e)\n",
    "        \n",
    "    cursorGaz.execute(\"\"\"SELECT DISTINCT altname FROM altname\"\"\")\n",
    "    unique_placenames = cursorGaz.fetchall()\n",
    "    unique_placenames = [r[0] for r in unique_placenames]\n",
    "    unique_placenames_array = list(set(list(np.array(unique_placenames))))\n",
    "    \n",
    "    # Close DB connection:\n",
    "    if (gazDB.is_connected()):\n",
    "        cursorGaz.close()\n",
    "        gazDB.close()\n",
    "    \n",
    "    return unique_placenames_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English WikiGazetteer:\n",
    "wiki_en_array = find_placenames(\"wikiGazetteer\")\n",
    "format_for_candranker(\"wikigaz_en\", wiki_en_array)\n",
    "\n",
    "# Spanish WikiGazetteer:\n",
    "wiki_es_array = find_placenames(\"wikiGazES\")\n",
    "format_for_candranker(\"wikigaz_es\", wiki_es_array)\n",
    "\n",
    "# Greek WikiGazetteer:\n",
    "wiki_el_array = find_placenames(\"wikiGazEL\")\n",
    "format_for_candranker(\"wikigaz_el\", wiki_el_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing Pleiades\n",
    "\n",
    "Download Pleiades gazetteer [from here](http://atlantides.org/downloads/pleiades/dumps/pleiades-names-latest.csv.gz) and store it in `toponym_matching/resources/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../resources/pleiades-names-latest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter only interested in entries written in Greek alphabet, and format them according to the format needed as input for DeezyMatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternatename = []\n",
    "pid = []\n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"nameLanguage\"] == \"grc\" or row[\"nameLanguage\"] == \"el\":\n",
    "        if type(row[\"nameAttested\"]) == str and type(row[\"reprLat\"]) == float and type(row[\"reprLong\"]):\n",
    "            toponym = row[\"nameAttested\"]\n",
    "            alternatename.append(toponym)\n",
    "            pid.append(row[\"pid\"])\n",
    "            lat.append(row[\"reprLat\"])\n",
    "            lon.append(row[\"reprLong\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleiades_gaz = pd.DataFrame()\n",
    "pleiades_gaz['altname'] = alternatename\n",
    "pleiades_gaz['pid'] = pid\n",
    "pleiades_gaz['lat'] = lat\n",
    "pleiades_gaz['lon'] = lon\n",
    "            \n",
    "pleiades_gaz['lat'] = pd.to_numeric(pleiades_gaz['lat'], errors = 'coerce')\n",
    "pleiades_gaz['lon'] = pd.to_numeric(pleiades_gaz['lon'], errors = 'coerce')\n",
    "pleiades_gaz.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleiades_gaz.to_pickle(\"../../datasets/gazetteers/pleiades.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(pleiades_gaz[\"altname\"]))))\n",
    "format_for_candranker(\"pleiades\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Combine Pleiades and WikiGazEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_wgaz_pleiades = pd.concat([pd.read_pickle(\"../../datasets/gazetteers/pleiades.pkl\"), pd.read_pickle(\"../../datasets/gazetteers/wikigaz_el.pkl\")])\n",
    "greek_wgaz_pleiades = greek_wgaz_pleiades.drop_duplicates(subset = ['altname', 'lat', 'lon'])\n",
    "greek_wgaz_pleiades.to_pickle(\"../../datasets/gazetteers/wikigaz_pleiades_el.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(greek_wgaz_pleiades[\"altname\"].unique()))))\n",
    "format_for_candranker(\"wikigaz_pleiades_el\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing HGIS de las Indias\n",
    "\n",
    "Do the following four steps only once:\n",
    "1. Download gazetteer from https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/FUSJD3/DK27GE&version=2.0\n",
    "2. Unzip and file in `toponym_matching/resources/`.\n",
    "3. Convert zip to df (uncomment and run cell below).\n",
    "4. Store dataframe in `toponym_matching/resources/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and run this only once. Change filename accordingly:\n",
    "# import simpledbf\n",
    "# dbf = simpledbf.Dbf5('../../resources/gazetteer-2019-03-28/gazetteer-2019-03-28.dbf')\n",
    "# df = dbf.to_dataframe()\n",
    "# df.to_pickle(\"../../resources/hgis_de_las_indias.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../resources/hgis_de_las_indias.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias = pd.DataFrame()\n",
    "df_indias[\"altname\"] = df[\"label\"]\n",
    "df_indias[\"pid\"] = df[\"gz_id\"]\n",
    "df_indias[\"lat\"] = df[\"lat\"]\n",
    "df_indias[\"lon\"] = df[\"lon\"]\n",
    "            \n",
    "df_indias['lat'] = pd.to_numeric(df_indias['lat'], errors = 'coerce')\n",
    "df_indias['lon'] = pd.to_numeric(df_indias['lon'], errors = 'coerce')\n",
    "df_indias.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias = df_indias.drop_duplicates(subset=[\"altname\", \"pid\", \"lat\", \"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indias.to_pickle(\"../../datasets/gazetteers/hgisindias.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(df_indias[\"altname\"]))))\n",
    "format_for_candranker(\"hgisindias\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Combine HGISindias and WikiGazES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wges = pd.read_pickle(\"../../datasets/gazetteers/wikigaz_es.pkl\")\n",
    "wges = wges.rename(columns={\"wikititle\": \"pid\"})\n",
    "\n",
    "es_wgaz_hgisindias = pd.concat([pd.read_pickle(\"../../datasets/gazetteers/hgisindias.pkl\"), wges])\n",
    "es_wgaz_hgisindias = es_wgaz_hgisindias.drop_duplicates(subset = ['altname', 'lat', 'lon'])\n",
    "es_wgaz_hgisindias.to_pickle(\"../../datasets/gazetteers/wikigaz_hgisindias_es.pkl\")\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(es_wgaz_hgisindias[\"altname\"]))))\n",
    "format_for_candranker(\"wikigaz_hgisindias_es\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37torch)",
   "language": "python",
   "name": "py37torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
