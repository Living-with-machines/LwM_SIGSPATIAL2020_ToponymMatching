{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets for external evaluation\n",
    "\n",
    "This notebook prepares the datasets that are going to be used for external evaluation. Each dataset is stored into a dataframe that has some columns that are the same in all the dataframes, plus some additional columns that are unique for this particular dataset.\n",
    "\n",
    "Columns in common in all dataframes are:\n",
    "- `source` (url or identifier of the document)\n",
    "- `text` (full text of the document)\n",
    "- `toponym` (each toponym as appears in the text)\n",
    "- `startCh` (start character of the toponym in the text)\n",
    "- `endCh` (end character of the toponym in the text)\n",
    "- `lat` (latitude of the resolved location)\n",
    "- `lon` (longitude of the resolved location)\n",
    "- `reference` (whether the main reference is Wikipedia identifier or coordinates. If the reference is Wikipedia, we've in any case derived its coordinates when possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the external datasets\n",
    "\n",
    "Download the following datasets and store them in `processing/resources/`:\n",
    "- [x] [The War of the Rebellion dataset](https://github.com/utcompling/WarOfTheRebellion/archive/master.zip) (unzip it)\n",
    "- [x] [La Argentina Manuscrita](https://recogito.pelagios.org/document/wzqxhk0h3vpikm/downloads) (Download the .csv file and rename it as `argentina_manuscrita.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import ast\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the War of the Rebellion test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wotr_coords(coords_string):\n",
    "    latitude = None\n",
    "    longitude = None\n",
    "    re_coords = r'.*Point.*coordinates\\\"\\:\\[\\s?(.*)\\t?\\,(.*)\\]\\}'\n",
    "    if re.match(re_coords, coords_string):\n",
    "        longitude, latitude = re.match(re_coords, coords_string).groups()\n",
    "        latitude = latitude.replace(\"\\t\", \"\").strip()\n",
    "        longitude = longitude.replace(\"\\t\", \"\").strip()\n",
    "    return latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wotr_split(input_directory, output_directory, json_split, split):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', 'volume', 'vol_charrange']\n",
    "    rows = []\n",
    "    numCoords = 0\n",
    "    for item in json_split:\n",
    "        volume = item['vol']\n",
    "        text = item['text'].replace(\"\\n\", \" \")\n",
    "        docid = item['docid']\n",
    "        vol_charrange = item['vol_charrange']\n",
    "        for ne in item['named_entities']:\n",
    "            startCh = ne['char_end']\n",
    "            endCh = ne['char_start']\n",
    "            toponym = ne['entity_string']\n",
    "            lat, lon = parse_wotr_coords(ne['geo'])\n",
    "            if lat and lon:\n",
    "                rows.append([docid, text, toponym, startCh, endCh, lat, lon, 'coordinates', volume, vol_charrange])\n",
    "    wotrcorpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    wotrcorpusdf.to_pickle(output_directory + \"wotr_\" + split + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wotr_corpus(input_directory, output_directory, corpus):\n",
    "    datasplit = ['test']\n",
    "    for split in datasplit:\n",
    "        with open(input_directory + corpus + 'Toponym/json/wotr-topo-' + split + '.json') as json_file:\n",
    "            json_split = json.load(json_file)\n",
    "            process_wotr_split(input_directory, output_directory, json_split, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_directory = \"../resources/\"\n",
    "output_directory = \"../../datasets/candidate_ranking_datasets/\"\n",
    "process_wotr_corpus(input_directory, output_directory, \"WarOfTheRebellion-master/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Argentina Manuscrita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_recogito(input_directory, output_directory, corpus):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', \"uri\", \"vocab\", \"comments\"]\n",
    "    rows = []\n",
    "    \n",
    "    df = pd.read_csv(input_directory + corpus)\n",
    "    df_places = df[(df[\"TYPE\"] == \"PLACE\") & (df[\"VERIFICATION_STATUS\"] == \"VERIFIED\")]\n",
    "    for i, row in df_places.iterrows():\n",
    "        rows.append([row[\"FILE\"], \"\", row[\"QUOTE_TRANSCRIPTION\"], row[\"ANCHOR\"].split(\":\")[-1], str(int(row[\"ANCHOR\"].split(\":\")[-1]) + len(row[\"QUOTE_TRANSCRIPTION\"])), row[\"LAT\"], row[\"LNG\"], \"coordinates\", row[\"URI\"], row[\"VOCAB_LABEL\"], row[\"COMMENTS\"]])\n",
    "    recogito_corpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    recogito_corpusdf.to_pickle(output_directory + corpus.split(\".\")[0] + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"../resources/\"\n",
    "output_directory = \"../../datasets/candidate_ranking_datasets/\"\n",
    "\n",
    "process_recogito(input_directory, output_directory, \"argentina_manuscrita.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37torch)",
   "language": "python",
   "name": "py37torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
