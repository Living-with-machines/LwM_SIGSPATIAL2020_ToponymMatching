{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets for external evaluation\n",
    "\n",
    "This notebook prepares the datasets that are going to be used for external evaluation. Each dataset is stored into a dataframe that has some columns that are the same in all the dataframes, plus some additional columns that are unique for this particular dataset.\n",
    "\n",
    "Columns in common in all dataframes are:\n",
    "- `source` (url or identifier of the document)\n",
    "- `text` (full text of the document)\n",
    "- `toponym` (each toponym as appears in the text)\n",
    "- `startCh` (start character of the toponym in the text)\n",
    "- `endCh` (end character of the toponym in the text)\n",
    "- `lat` (latitude of the resolved location)\n",
    "- `lon` (longitude of the resolved location)\n",
    "- `reference` (whether the main reference is Wikipedia identifier or coordinates. If the reference is Wikipedia, we've in any case derived its coordinates when possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the external datasets\n",
    "\n",
    "Download the following datasets and store them in the `datasets` folder:\n",
    "- [x] [The War of the Rebellion dataset](https://github.com/utcompling/WarOfTheRebellion/archive/master.zip) (unzip it)\n",
    "- [x] [GeoVirus](https://github.com/milangritta/Pragmatic-Guide-to-Geoparsing-Evaluation/blob/master/data/Corpora/GeoVirus.xml)\n",
    "- [x] [TR-News](https://github.com/milangritta/Pragmatic-Guide-to-Geoparsing-Evaluation/blob/master/data/Corpora/TR-News.xml)\n",
    "- [x] [Local Global Lexicon](https://github.com/milangritta/Pragmatic-Guide-to-Geoparsing-Evaluation/blob/master/data/Corpora/lgl.xml)\n",
    "- [x] [GeoWebNews](https://github.com/milangritta/Pragmatic-Guide-to-Geoparsing-Evaluation/blob/master/data/GWN.xml)\n",
    "- [x] [La Argentina Manuscrita](https://recogito.pelagios.org/document/wzqxhk0h3vpikm/downloads) (Download the .csv file and rename it as `argentina_manuscrita.csv`)\n",
    "- [x] [Pausanias: Periegesis](https://recogito.pelagios.org/document/35zv4zm4iqp4aw/downloads) (Download the .csv file and rename it as `pausanias_periegesis.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from conllu import parse\n",
    "from conllu import parse_tree\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import ast\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the external datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geovirus_corpus(corpus):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', 'wikititle']\n",
    "    rows = []\n",
    "    tree = ET.parse(corpus)\n",
    "    root = tree.getroot()\n",
    "    for article in root:\n",
    "        source = article.find('./source').text\n",
    "        text = article.find('./text').text\n",
    "        for location in article.findall('./locations/location'):\n",
    "            toponym = location.find('name').text\n",
    "            startCh = location.find('start').text\n",
    "            endCh = location.find('end').text\n",
    "            lat = location.find('lat').text\n",
    "            lon = location.find('lon').text\n",
    "            wikititle = location.find('page').text.split(\"/\")[-1]\n",
    "            rows.append([source, text, toponym, startCh, endCh, lat, lon, 'wikipedia', wikititle])\n",
    "    xmlcorpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    xmlcorpusdf.to_pickle(\"../../datasets/extrinsic/\" + corpus.split('.xml')[0].split(\"/\")[-1] + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lgl_corpus(corpus):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', 'geoname', 'geonameid', 'fclass', 'fcode', 'feedid', 'title', 'dltime', 'domain']\n",
    "    rows = []\n",
    "    tree = ET.parse(corpus)\n",
    "    root = tree.getroot()\n",
    "    for article in root:\n",
    "        text = article.find('./text').text\n",
    "        feedid = article.find('./feedid').text\n",
    "        url = article.find('./url').text\n",
    "        title = article.find('./title').text\n",
    "        domain = article.find('./domain').text\n",
    "        dltime = article.find('./dltime').text\n",
    "        for location in article.findall('./toponyms/toponym'):\n",
    "            toponym = location.find('phrase').text\n",
    "            startCh = location.find('start').text\n",
    "            endCh = location.find('end').text\n",
    "            gaztag = location.find('gaztag')\n",
    "            geonameid = \"\"\n",
    "            locname = \"\"\n",
    "            fclass = \"\"\n",
    "            fcode = \"\"\n",
    "            lat = \"\"\n",
    "            lon = \"\"\n",
    "            if gaztag:\n",
    "                geonameid = gaztag.attrib['geonameid']\n",
    "                locname = gaztag.find('./name').text\n",
    "                fclass = gaztag.find('./fclass').text\n",
    "                fcode = gaztag.find('./fcode').text\n",
    "                lat = gaztag.find('./lat').text\n",
    "                lon = gaztag.find('./lon').text\n",
    "            rows.append([url, text, toponym, startCh, endCh, lat, lon, 'coordinates', locname, geonameid, fclass, fcode, feedid, title, dltime, domain])\n",
    "    print(len(rows))\n",
    "    xmlcorpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    print(xmlcorpusdf.shape)\n",
    "    xmlcorpusdf.to_pickle(\"../../datasets/extrinsic/\" + corpus.split('.xml')[0].split(\"/\")[-1] + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_trnews_corpus(corpus):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', 'geoname', 'geonameid', 'fclass', 'fcode', 'title', 'dltime', 'domain']\n",
    "    rows = []\n",
    "    tree = ET.parse(corpus)\n",
    "    root = tree.getroot()\n",
    "    for article in root:\n",
    "        text = article.find('./text').text\n",
    "        url = article.find('./url').text\n",
    "        title = article.find('./title').text\n",
    "        domain = article.find('./domain').text\n",
    "        dltime = article.find('./dltime').text\n",
    "        for location in article.findall('./toponyms/toponym'):\n",
    "            toponym = location.find('phrase').text\n",
    "            startCh = location.find('start').text\n",
    "            endCh = location.find('end').text\n",
    "            gaztag = location.find('gaztag')\n",
    "            geonameid = \"\"\n",
    "            locname = \"\"\n",
    "            fclass = \"\"\n",
    "            fcode = \"\"\n",
    "            lat = \"\"\n",
    "            lon = \"\"\n",
    "            if gaztag:\n",
    "                geonameid = gaztag.attrib['geonameid']\n",
    "                locname = gaztag.find('./name').text\n",
    "                fclass = gaztag.find('./fclass')\n",
    "                fcode = gaztag.find('./fcode')\n",
    "                fclass = fclass.text if not fclass == None else \"\"\n",
    "                fcode = fcode.text if not fcode == None else \"\"\n",
    "                lat = gaztag.find('./lat').text\n",
    "                lon = gaztag.find('./lon').text\n",
    "            rows.append([url, text, toponym, startCh, endCh, lat, lon, 'coordinates', locname, geonameid, fclass, fcode, title, dltime, domain])\n",
    "    xmlcorpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    xmlcorpusdf.to_pickle(\"../../datasets/extrinsic/\" + corpus.split('.xml')[0].split(\"/\")[-1] + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gwn_corpus(corpus):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', 'geoname', 'geonameid', 'title']\n",
    "    rows = []\n",
    "    tree = ET.parse(corpus)\n",
    "    root = tree.getroot()\n",
    "    for article in root:\n",
    "        text = article.find('./text').text\n",
    "        url = article.find('./link').text\n",
    "        title = article.find('./title').text\n",
    "        for location in article.findall('./toponyms/toponym'):\n",
    "            toponym = location.find('extractedName').text\n",
    "            startCh = location.find('start').text\n",
    "            endCh = location.find('end').text\n",
    "            locname = location.find('normalisedName')\n",
    "            locname = locname.text if not locname == None else \"\"\n",
    "            loctype = location.find('type')\n",
    "            loctype = loctype.text if not loctype == None else \"\"\n",
    "            geonameid = location.find('geonamesID')\n",
    "            geonameid = geonameid.text if not geonameid == None else \"\"\n",
    "            lat = location.find('latitude')\n",
    "            lat = lat.text if not lat == None else \"\"\n",
    "            lon = location.find('longitude')\n",
    "            lon = lon.text if not lon == None else \"\"\n",
    "            if lat and lon:\n",
    "                rows.append([url, text, toponym, startCh, endCh, lat, lon, 'coordinates', locname, geonameid, title])\n",
    "    xmlcorpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    xmlcorpusdf.to_pickle(\"../../datasets/extrinsic/\" + corpus.split('.xml')[0].split(\"/\")[-1] + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wotr_coords(coords_string):\n",
    "    latitude = None\n",
    "    longitude = None\n",
    "    re_coords = r'.*Point.*coordinates\\\"\\:\\[\\s?(.*)\\t?\\,(.*)\\]\\}'\n",
    "    if re.match(re_coords, coords_string):\n",
    "        longitude, latitude = re.match(re_coords, coords_string).groups()\n",
    "        latitude = latitude.replace(\"\\t\", \"\").strip()\n",
    "        longitude = longitude.replace(\"\\t\", \"\").strip()\n",
    "    return latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wotr_split(directory, json_split, split):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', 'volume', 'vol_charrange']\n",
    "    rows = []\n",
    "    numCoords = 0\n",
    "    for item in json_split:\n",
    "        volume = item['vol']\n",
    "        text = item['text'].replace(\"\\n\", \" \")\n",
    "        docid = item['docid']\n",
    "        vol_charrange = item['vol_charrange']\n",
    "        for ne in item['named_entities']:\n",
    "            startCh = ne['char_end']\n",
    "            endCh = ne['char_start']\n",
    "            toponym = ne['entity_string']\n",
    "            lat, lon = parse_wotr_coords(ne['geo'])\n",
    "            if lat and lon:\n",
    "                rows.append([docid, text, toponym, startCh, endCh, lat, lon, 'coordinates', volume, vol_charrange])\n",
    "    wotrcorpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    wotrcorpusdf.to_pickle(\"../../datasets/extrinsic/wotr_\" + split + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wotr_corpus(directory, corpus):\n",
    "    datasplit = ['train', 'test']\n",
    "    for split in datasplit:\n",
    "        with open(directory + corpus + 'Toponym/json/wotr-topo-' + split + '.json') as json_file:\n",
    "            json_split = json.load(json_file)\n",
    "            process_wotr_split(directory, json_split, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5088\n",
      "(5088, 16)\n"
     ]
    }
   ],
   "source": [
    "directory = \"ext_datasets/\"\n",
    "process_geovirus_corpus(directory + \"GeoVirus.xml\")\n",
    "process_lgl_corpus(directory + \"lgl.xml\")\n",
    "process_trnews_corpus(directory + \"TR-News.xml\")\n",
    "process_gwn_corpus(directory + \"GWN.xml\")\n",
    "process_wotr_corpus(directory, \"WarOfTheRebellion-master/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source (url or identifier of the document)\n",
    "# text (full text of the document)\n",
    "# toponym (each toponym as appears in the text)\n",
    "# startCh (start character of the toponym in the text)\n",
    "# endCh (end character of the toponym in the text)\n",
    "# lat (latitude of the resolved location)\n",
    "# lon (longitude of the resolved location)\n",
    "# reference (whether the main reference is Wikipedia identifier or coordinates. If the reference is Wikipedia, we've in any case derived its coordinates when possible)\n",
    "\n",
    "def process_recogito(directory, corpus):\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'reference', \"uri\", \"vocab\", \"comments\"]\n",
    "    rows = []\n",
    "    \n",
    "    df = pd.read_csv(directory + corpus)\n",
    "    df_places = df[(df[\"TYPE\"] == \"PLACE\") & (df[\"VERIFICATION_STATUS\"] == \"VERIFIED\")]\n",
    "    for i, row in df_places.iterrows():\n",
    "        rows.append([row[\"FILE\"], \"\", row[\"QUOTE_TRANSCRIPTION\"], row[\"ANCHOR\"].split(\":\")[-1], str(int(row[\"ANCHOR\"].split(\":\")[-1]) + len(row[\"QUOTE_TRANSCRIPTION\"])), row[\"LAT\"], row[\"LNG\"], \"coordinates\", row[\"URI\"], row[\"VOCAB_LABEL\"], row[\"COMMENTS\"]])\n",
    "    recogito_corpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    recogito_corpusdf.to_pickle(\"../../datasets/extrinsic/\" + corpus.split(\".\")[0] + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_recogito(directory, \"argentina_manuscrita.csv\")\n",
    "process_recogito(directory, \"pausanias_periegesis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the FMP dataset annotated by LwM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, to run this notebook you will need the annotations created by the LwM project and a local version of WikiGazetteer. To build a WikiGazetteer (into a MySQL database) for a specific Wikipedia language and version follow [these instructions](https://github.com/Living-with-machines/lwm_GIR19_resolving_places/tree/master/gazetteer_construction). Make sure you change your credentials in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "gazDB = \"\"\n",
    "cursorGaz = \"\"\n",
    "try:\n",
    "    gazDB = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            database='gazetteer',\n",
    "            user='testGazetteer',\n",
    "            password='1234')\n",
    "    if gazDB.is_connected():\n",
    "        cursorGaz = gazDB.cursor(dictionary=True)\n",
    "except Error as e:\n",
    "    print(\"Error while connecting to MySQL\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_coords(wikititle):\n",
    "    if gazetteerDB_server == \"mysqlGaz\":\n",
    "        cursorGaz.execute(\"\"\"\n",
    "            SELECT lat, lon FROM location\n",
    "            where wiki_title=%s\n",
    "        \"\"\", (wikititle,))\n",
    "    results = cursorGaz.fetchall()\n",
    "    if len(results) >= 1:\n",
    "        lat = results[0][\"lat\"]\n",
    "        lon = results[0][\"lon\"]\n",
    "        return lat, lon\n",
    "    else:\n",
    "        return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fmp_corpus(directory, corpus):\n",
    "    retoken = r'^[0-9]+\\-[0-9]+\\t([0-9]+)\\-([0-9]+)\\t([^\\t]+)\\t([^\\t]+)\\t(.*)$'\n",
    "    reIndex = r'(.*)\\[([0-9]+)\\]'\n",
    "    rows = []\n",
    "    cols = ['source', 'text', 'toponym', 'startCh', 'endCh', 'lat', 'lon', 'wikititle', 'reference', 'loctype', 'publplace', 'publdate']\n",
    "    for i in glob.glob(directory + corpus + \"*/annotation/*/*\"):\n",
    "        if i.split(\"/\")[-1] == \"mariona.tsv\":\n",
    "            fileid = i.split(\"/\")[-2]\n",
    "            publplace = fileid.split(\"_\")[1].split(\".txt\")[0][:-4]\n",
    "            publdate = fileid.split(\"_\")[1].split(\".txt\")[0][-4:]\n",
    "            \n",
    "            text = \"\"\n",
    "            with open(directory + corpus + \"/\" + i.split(\"/\")[-4] + \"/source/\" + i.split(\"/\")[-2]) as fr:\n",
    "                text = fr.read()\n",
    "            \n",
    "            dMultigrams = dict()\n",
    "            with open(i) as fr:\n",
    "                lines = fr.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if re.match(retoken, line):\n",
    "                        startCh, endCh, toponym, wikiurl, loctype = re.match(retoken, line).groups()\n",
    "                        wikiurl = urllib.parse.unquote(wikiurl).replace(\"\\\\\", \"\")\n",
    "                        if \"—\" in toponym:\n",
    "                            toponym = toponym.split(\"—\")[0]\n",
    "                            endCh = str(int(startCh) + len(toponym))\n",
    "                        if 'wiki' in wikiurl and loctype.lower() == 'locwiki':\n",
    "                            wikiurl = wikiurl.split(\"/\")[-1]\n",
    "                            if re.match(reIndex, loctype):\n",
    "                                wikiurl, entindex = re.match(reIndex, wikiurl).groups()\n",
    "                                if entindex in dMultigrams:\n",
    "                                    dMultigrams[entindex].append((toponym, wikiurl, startCh, endCh, loctype))\n",
    "                                else:\n",
    "                                    dMultigrams[entindex] = [(toponym, wikiurl, startCh, endCh, loctype)]\n",
    "                            # If the entity is just one token:        \n",
    "                            else:\n",
    "                                lat, lon = get_wiki_coords(wikiurl)\n",
    "                                rows.append((fileid, text, toponym, startCh, endCh, lat, lon, wikiurl, \"wikipedia\", loctype, publplace, publdate))\n",
    "            \n",
    "            for entindex in dMultigrams:\n",
    "                multitoken_startCh = int(dMultigrams[entindex][0][2])\n",
    "                multitoken_endCh = int(dMultigrams[entindex][-1][3])\n",
    "                multitoken_toponym = text[multitoken_startCh:multitoken_endCh]\n",
    "                multitoken_url = dMultigrams[entindex][0][1].split(\"/\")[-1]\n",
    "                multitoken_loctype = dMultigrams[entindex][0][-1].split(\"[\")[0]\n",
    "                lat, lon = get_wiki_coords(multitoken_url)\n",
    "                rows.append((fileid, text, multitoken_toponym, multitoken_startCh, multitoken_endCh, lat, lon, multitoken_url, \"wikipedia\", multitoken_loctype, publplace, publdate))\n",
    "    fmpcorpusdf = pd.DataFrame(rows, columns = cols)\n",
    "    fmpcorpusdf.to_pickle(\"../../datasets/extrinsic/fmp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"ext_datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_fmp_corpus(directory, \"ToponymResolutionGoldStandard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gazetteerDB_server == \"mysqlGaz\":\n",
    "    if (gazDB.is_connected()):\n",
    "        cursorGaz.close()\n",
    "        gazDB.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
