{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import editops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load token-level alignments\n",
    "\n",
    "Load token-level alignments between OCR and their corresponding corrections by a human. The file `ocrTokens.tsv` is the output from running `extract_ocr_alignments.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrdf = pd.read_csv(\"../../../resources/ocrTokens.tsv\", sep=\"\\t\", names=[\"ocrtext\", \"humtext\"])\n",
    "ocrdf = ocrdf.drop_duplicates(subset=['ocrtext', 'humtext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture transformations\n",
    "\n",
    "Given an OCR'd token and its correction, capture the edit operations needed to go from one to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_ranges(ltrans):\n",
    "    transchunks = []\n",
    "    for k, g in groupby(enumerate(ltrans), lambda ix : ix[0] - ix[1]):\n",
    "        transchunks.append(list(map(itemgetter(1), g)))\n",
    "    return transchunks\n",
    "\n",
    "def capture_transformations(humtext, ocrtext, transformations):\n",
    "    observed_transformations = []\n",
    "    \n",
    "    allreplaces = True # True if all transformations are 'replace'\n",
    "    for t in transformations:\n",
    "        if t[0] != 'replace':\n",
    "            allreplaces = False\n",
    "\n",
    "    # Dictionary of human2ocr transformations, and of ocr2hum transformations\n",
    "    dHum2OcrTrans = dict()\n",
    "    dOcr2HumTrans = dict()\n",
    "    for t in transformations:\n",
    "        ttype = t[0]\n",
    "        tcharhum = t[1]\n",
    "        tcharocr = t[2]\n",
    "        if tcharhum in dHum2OcrTrans:\n",
    "            dHum2OcrTrans[tcharhum].append((tcharocr, ttype))\n",
    "        else:\n",
    "            dHum2OcrTrans[tcharhum] = [(tcharocr, ttype)]\n",
    "        if tcharocr in dOcr2HumTrans:\n",
    "            dOcr2HumTrans[tcharocr].append((tcharhum, ttype))\n",
    "        else:\n",
    "            dOcr2HumTrans[tcharocr] = [(tcharhum, ttype)]\n",
    "        \n",
    "    # Only 'replace' transformations:\n",
    "    if allreplaces == True:\n",
    "        humrepls = [t[1] for t in transformations] # Indices replaced in humtext\n",
    "        ocrrepls = [t[2] for t in transformations] # Indices replaced in ocrtext\n",
    "        humranges = find_consecutive_ranges(humrepls)\n",
    "        ocrranges = find_consecutive_ranges(ocrrepls)\n",
    "        \n",
    "        for i in range(len(humranges)):\n",
    "            newhum = \"\"\n",
    "            newocr = \"\"\n",
    "            for j in range(len(humranges[i])):\n",
    "                newhum += humtext[humranges[i][j]]\n",
    "                newocr += ocrtext[ocrranges[i][j]]\n",
    "            if newhum or newocr:\n",
    "                observed_transformations.append((newhum, newocr))\n",
    "    \n",
    "    # Other transformations too:\n",
    "    else:\n",
    "        # If there is a 'delete', we capture it from the ocr2hum dictionary\n",
    "        for ocr in dOcr2HumTrans:\n",
    "            newhum = \"\"\n",
    "            newocr = \"\"\n",
    "            try:\n",
    "                newocr = ocrtext[ocr]\n",
    "                if len(dOcr2HumTrans[ocr]) > 1 and any(tr_item[1] == 'delete' for tr_item in dOcr2HumTrans[ocr]):\n",
    "                    for ch_trans in dOcr2HumTrans[ocr]:\n",
    "                        newhum += humtext[ch_trans[0]]\n",
    "                if newhum and newocr:\n",
    "                    observed_transformations.append((newhum, newocr))\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "        # If there is an 'insert', we capture it from the hum2ocr dictionary    \n",
    "        for hum in dHum2OcrTrans:\n",
    "            newhum = \"\"\n",
    "            newocr = \"\"\n",
    "            try:\n",
    "                newocr = humtext[hum]\n",
    "                if len(dHum2OcrTrans[hum]) > 1 and any(tr_item[1] == 'insert' for tr_item in dHum2OcrTrans[hum]):\n",
    "                    for ch_trans in dHum2OcrTrans[hum]:\n",
    "                        newhum += ocrtext[ch_trans[0]]\n",
    "                if newhum and newocr:\n",
    "                    observed_transformations.append((newhum, newocr))\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return list(set(observed_transformations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main resulting structures:\n",
    "\n",
    "* `dTransformations`: dictionary where the keys are characters that have been transformed (from human to OCR token) at least once. The value is a list of characters representing all its corresponding transformations in the OCR'd tokens:\n",
    "\n",
    "```\n",
    "'b': ['u','t','t','r','h','o','o','h','h','t','h','h','n','h','h','h','l','h','h','h''h','h','h','L','h',...]\n",
    "```\n",
    "\n",
    "* `keepTransformedTokens`: dictionary of dictionaries where the outer keys are characters that have been transformed (from human to OCR token) at least once. The inner keys are all characters into which the outer keys have been transformed, with the inner values being a list of pair tuples, in which the first element is the human correction and the second token the OCR'd version of it:\n",
    "\n",
    "```\n",
    "{'b': {\n",
    "    {'u': [('Public', 'Puulic'),\n",
    "      ('Erebendary', 'Ereuendary'),\n",
    "      ('Abigail', 'Auigail'),\n",
    "      ('Cutbush', 'Cutuush'),\n",
    "      ('Glebe', 'Gleue'),\n",
    "      ('Venb', 'Venu'),\n",
    "      ('Gibbs', 'Giubs'),\n",
    "      ('Elbra', 'Elura'),\n",
    "      ('Baber', 'Bauer')],\n",
    "     't': [('Labour', 'Latour'),\n",
    "      ('Rugby', 'Rugty'),\n",
    "      ('Cumberland', 'Cumterland'),\n",
    "      ('Webb', 'Webt'),\n",
    "      ('Grubb', 'Grubt'),\n",
    "      ('Barbour', 'Bartour'),\n",
    "      ('Robert', 'Rotert'),\n",
    "      ('Liberal', 'Literal')],\n",
    "     ...},\n",
    "...}\n",
    "```\n",
    "\n",
    "We take into consideration multiple character transformations, e.g.:\n",
    "```\n",
    "{'rn':\n",
    "    {'m': [('Amsberg', 'Arnsberg'),\n",
    "      ('Commonwealth', 'Cornmonweillh'),\n",
    "      ('Harney', 'Hamey'),\n",
    "      ('Palembang', 'Palernbang'),\n",
    "      ('Duma', 'Durna'),\n",
    "      ...},\n",
    "    {'ni': [('Purnell', 'Puniell'),\n",
    "      ('Whitbourne', 'Whitbounie'),\n",
    "      ('Murnin', 'Muniin'),\n",
    "      ('Pangborn', 'Pangboni'),\n",
    "      ('Clibborn', 'Clibboni'),\n",
    "      ...},\n",
    " ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dTransformations = dict()\n",
    "keepTransformedTokens = dict()\n",
    "for i, row in ocrdf.iterrows():\n",
    "    ocrtext = str(row['ocrtext']).strip()\n",
    "    humtext = str(row['humtext']).strip()\n",
    "    edits = editops(humtext, ocrtext)\n",
    "    captured_transformations = capture_transformations(humtext, ocrtext, edits)\n",
    "    # we keep only pairs where we detect one type of transformation\n",
    "    if len(captured_transformations) == 1:\n",
    "        t = captured_transformations[0]\n",
    "        if t[0] in keepTransformedTokens:\n",
    "            if t[1] in keepTransformedTokens[t[0]]:\n",
    "                keepTransformedTokens[t[0]][t[1]].append((humtext, ocrtext))\n",
    "            else:\n",
    "                keepTransformedTokens[t[0]][t[1]] = [(humtext, ocrtext)]\n",
    "            dTransformations[t[0]].append(t[1])\n",
    "        else:\n",
    "            keepTransformedTokens[t[0]] = {t[1]: [(humtext, ocrtext)]}\n",
    "            dTransformations[t[0]] = [t[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create true variations pairs\n",
    "\n",
    "We create true variation pairs if the transformation in them appears more than a `threshold` number of times in the aligned tokens (set to 1).\n",
    "\n",
    "The result is `true_variations`, a dictionary in which keys are human-corrected tokens and their values are lists of their OCR'd tokens with errors:\n",
    "\n",
    "```\n",
    "{'Customline': ['Customlinc'],\n",
    " 'Caspers': ['Caspcrs', 'Caspeis', 'Csspers', 'Cnspers', 'Caspera', 'Cappers', 'Gaspers'],\n",
    " 'Porteous': ['Portcous', 'Portoous', \"I'ortcous\"],\n",
    " 'Jagelman': ['Jagclman', 'Jagelmau', 'Jngelman', 'Jagelmnn'],\n",
    " ...}\n",
    "```\n",
    "\n",
    "These are all transformations that occur in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "\n",
    "common_charpairs = dict()\n",
    "for t in keepTransformedTokens:\n",
    "    countTransf = keepTransformedTokens[t]\n",
    "    dTransfInst = dict([(k, len(countTransf[k])) for k in countTransf if len(countTransf[k]) > threshold])\n",
    "    if dTransfInst:\n",
    "        common_charpairs[t] = list(dTransfInst.keys())\n",
    "        \n",
    "true_variations = dict()\n",
    "for k in common_charpairs:\n",
    "    for v in common_charpairs[k]:\n",
    "        tok_tuples = keepTransformedTokens[k][v]\n",
    "        for t in tok_tuples:\n",
    "            if t[0] in true_variations:\n",
    "                true_variations[t[0]].append(t[1])\n",
    "            else:\n",
    "                true_variations[t[0]] = [t[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create false variations pairs\n",
    "\n",
    "For each distinct human-corrected token, we artificially create as many false variation pairs as there are true variations. We create negative pairs by replacing 1-, 2-, or 3-gram characters in the human-corrected token by characters that have not been observed as a possible variation to this character in our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_false_variations(numTransf, hum_token, dTransformations, allTransfKeys, allTransfValues, seenTuples):\n",
    "    if numTransf == \"random\":\n",
    "        random_subst_len = random.choice([1, 2, 3])\n",
    "    else:\n",
    "        random_subst_len = numTransfs\n",
    "        \n",
    "    parts = [hum_token[i : i + random_subst_len] for i in range(0, len(hum_token), random_subst_len)]\n",
    "    random.shuffle(allTransfValues)\n",
    "    random.shuffle(parts)\n",
    "    keep_false_variation = \"\"\n",
    "    if parts:\n",
    "        for p in parts:\n",
    "                for tv in allTransfValues:\n",
    "                    \n",
    "                    if len(tv) <= 3:\n",
    "                        if not (p, tv) in seenTuples and not (tv, p) in seenTuples:\n",
    "                            keep_false_variation = hum_token.replace(p, tv, 1)\n",
    "\n",
    "                    if keep_false_variation.strip() != \"\":\n",
    "                        break\n",
    "                        \n",
    "    return keep_false_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# maximum length of the transformed string, by default is set to 3\n",
    "max_len_transformations = 3\n",
    "\n",
    "transformations_keys = [transf for transf in list(dTransformations.keys()) if len(transf) <= max_len_transformations]\n",
    "transformations_values = [transf for transf in list(dTransformations.values())]\n",
    "transformations_values = list(set([item for sublist in transformations_values for item in sublist]))\n",
    "\n",
    "seenTuples = set()\n",
    "for k in dTransformations:\n",
    "    for v in dTransformations[k]:\n",
    "        seenTuples.add((k, v))\n",
    "        seenTuples.add((v, k))\n",
    "\n",
    "with open('ocr_posneg.tsv', mode='w') as fw:\n",
    "    for hum_token in true_variations:\n",
    "        for i in range(len(true_variations[hum_token])):\n",
    "            truevar = true_variations[hum_token][i]\n",
    "            falsevar = create_false_variations(\"random\", hum_token, dTransformations, transformations_keys, transformations_values, seenTuples).strip()\n",
    "            if not falsevar:\n",
    "                falsevar = create_false_variations(1, hum_token, dTransformations, transformations_keys, transformations_values, seenTuples).strip()\n",
    "            if truevar and falsevar:\n",
    "                fw.write(hum_token.strip() + \"\\t\" + truevar  + \"\\tTRUE\\n\")\n",
    "                fw.write(hum_token.strip() + \"\\t\" + falsevar + \"\\tFALSE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37torch)",
   "language": "python",
   "name": "py37torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
